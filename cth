#!/usr/bin/perl

# --------------------------------------------------------------------------------------------
# cth: Contract Test Harness for Antelope
#
# Run "cth -h" for help.
#
# Project repository: https://github.com/fcecin/cth
#
# cth runs all the tests or a subset of them.
#
# cth initializes all tools and drivers if run with "cth -i".
# --------------------------------------------------------------------------------------------
#
# Driver development tips:
#
# The optional 'install' script that be in the driver's directory should be used to for
#   example one-time compile any contracts that the driver will be deploying over and
#   over again to a blockchain. Any one-time task, done after cloning the test harness
#   is good to go here.
#
# The optional 'reset' script in a driver should clean it up to a default state wherein
#   you e.g. would be required to 'install' it again afterwards. This could for example
#   clean/delete the build directories of contracts.
#
# Drivers should start with something like a 'start' script, and be stopped with
#   something like a 'stop' script. Start/stop (or whatever other) driver controls are
#   not required by cth; instead these are the interface called by each test to control
#   the driver(s) that it uses. Parallel test runs require a driver that can take a
#   command from a test that gives the test itself as e.g. the blockchain data directory.
#   Other tests may require a driver that is smart enough to preserve the same running
#   blockchain and contract deployment between tests (for whatever reason).
#
# A 'clear' script should usually require the driver to be "stopped" (or force stop),
#   since it should deal with wiping the driver's data files (such as blockchain state
#   generated by test activity.
#
# Drivers that need global configuration before they are run can provide a 'configure'
#   tool. The configure tool can be anything the driver needs. This is also not called
#   by cth and concerns only the tests that use it. Alternatively, driver scripts can
#   take command-line parameters, which can be accessed by using cth --run.
#
# --------------------------------------------------------------------------------------------
#
# IMPORTANT: cth recognizes the following exit codes from test programs:
#
#   0      Test has PASSED.
#
#   32     Test has SKIPPED itself.
#
#          This is returned by a non-crashing test when the test decides, based on switches
#            and options received from its command-line arguments (which are fed from cth),
#            that, although it has not been filtered out by the test name filter, it should
#            exclude itself from this run. Skipped tests are counted towards the total.
#
#   OTHER  Test has FAILED.
#
#          ANY return value other than 0 or 32 means the test has failed.
#          The returned value is the raw error code returned by the program. cth will print
#            the exact code returned by each test program to aid in debugging.
#          This can change; the rest of the 32-63 range will be leveraged for other special
#            cases that we may need to handle in cth.
#
# --------------------------------------------------------------------------------------------

$| = 1;

use strict;
use warnings;
use Getopt::Long;
use Cwd;
use File::Basename;
use File::Spec::Functions;

my $RED = "\e[31m";
my $GRN = "\e[32m";
my $YEL = "\e[33m";
my $DEF = "\e[0m";

# --------------------------------------------------------------------------------------------
# Tee all output to cth.log
# --------------------------------------------------------------------------------------------

my $log_file = "cth.log";
if (-e $log_file) { unlink $log_file or die "ERROR: cth: Could not delete '$log_file': $!"; }
open(STDOUT, "| tee -a $log_file") or die "ERROR: cth: Can't open '$log_file': $!"; # Redirect STDOUT to tee
open(STDERR, ">&STDOUT"); # Redirect STDERR to STDOUT

# --------------------------------------------------------------------------------------------
# Variables
# --------------------------------------------------------------------------------------------

my $failed_summary_log_file = "failed.log";

my $cth_exit_code_skipped_test = 32;

my $root_dir = getcwd();

my $default_tests_dir = "tests";

my $drivers_dir = "drivers";

# Uniform parameterization of the ENTIRE test suite that is run.
# -s and -o also allow cth to parameterize all driver installs via -i.
my $global_test_or_installer_args = '';

# This is a list of all tests directories. It's the same as @opt_testsdirs, but in absolute path form.
my @tests_dirs;

# Total tests found (scalar(@tests))
# This is updated after test name filtering (see @filtered_tests) to contain only the test count
#   after the filtering;
my $tests_size = 0;

# This is a list of all tests that will be run, with the full path to them.
# The test directory can be obtained via use basename($test), and the path via dirname($test).
# This array is *shortened* after it is first constructed, when the test name filter is applied to it.
my @tests;

# When the @tests array is name-filtered, the filtered tests that are removed from @tests are placed here.
my @filtered_tests;

# Common path prefix among all entries in @tests (to help reduce on-screen clutter)
my $tests_prefix;

my @drivers;         # All subdirectories under drivers/

my @testspecs;       # List of testspecs passed on the command line

my $sum_total = 0;
my $sum_fail = 0;
my $sum_pass = 0;
my $sum_skip = 0;

my @sum_passed_tests;
my @sum_failed_tests;
my @sum_skipped_tests;

# Run tests in parallel (child processes)
my $active_jobs = 0;   # Number of active child processes that we still need to wait() for
my %child_pids;        # To keep track of child PIDs and their corresponding test names

# flag to signal that tests will not run.
# tests only run if no set up tasks have been performed.
my $DO_NOT_RUN_TESTS = 0;

# if any setup/instalation task fails, set this so that tests will refuse to run
my @FAILED_SETUP_TASKS;

# --------------------------------------------------------------------------------------------
# Find out the default number of parallel jobs
# --------------------------------------------------------------------------------------------

my $cpus = `lscpu -b -p=Core,Socket | grep -v '^#' | sort -u | wc -l`;
chomp($cpus);
if (!defined $cpus || !($cpus =~ /^\d+$/)) {
    $cpus = 1;
}

# --------------------------------------------------------------------------------------------
# Command-line parser
# --------------------------------------------------------------------------------------------

# Options
my @opt_option;       # each element of these two outer arrays is a
my @opt_switch;       #  2-element array that contains the key and value pair.
my $opt_clear = 0;
my $opt_dump = 0;
my $opt_findtests;
my $opt_force = 0;
my $opt_help = 0;
my $opt_install = 0;
my $opt_jobs = $cpus;
my $opt_reset = 0;
my @opt_run;
my @opt_testsdir;
my $opt_verbose = 0;

GetOptions(
    "clear|c"      => \$opt_clear,
    "dump"         => \$opt_dump,
    "findtests=s"  => \$opt_findtests,
    "force|f"      => \$opt_force,
    "help|h"       => \$opt_help,
    "install|i"    => \$opt_install,
    "jobs|j=s"     => \$opt_jobs,
    "option|o=s%"  => sub { push @opt_option, [$_[1], $_[2]] },
    "reset"        => \$opt_reset,
    "run=s{3}"     => sub { push @opt_run, $_[1]; },
    "switch|s=s%"  => sub { push @opt_switch, [$_[1], $_[2]] },
    "testsdir|t=s" => sub { push @opt_testsdir, $_[1]; },
    "verbose|v"    => \$opt_verbose

) or die "ERROR: cth: Error in command line arguments.\n";

# keys must be azAZ09_ only
foreach my $pair (@opt_switch) {
    my ($key, $value) = @$pair;
    unless ($key =~ /^[a-zA-Z0-9_]+$/) {
        die "ERROR: Invalid characters in test switch key (allowed: a-z, A-Z, 0-9, _): \"$key\"\n";
    }
}
foreach my $pair (@opt_option) {
    my ($key, $value) = @$pair;
    unless ($key =~ /^[a-zA-Z0-9_]+$/) {
        die "ERROR: Invalid characters in test option key (allowed: a-z, A-Z, 0-9, _): \"$key\"\n";
    }
}

# jobs must be a positive integer
if (!($opt_jobs =~ /^\d+$/) || $opt_jobs < 1) {
    die "ERROR: invalid --jobs: $opt_jobs\n";
}

@testspecs = @ARGV;

if ($opt_help) {
    #      ................................................................................
    print "cth: Contract Test Harness for Antelope\n";
    print "\n";
    print "Usage:\n";
    print "  cth [OPTIONS] [TESTSPECS]\n";
    print "\n";
    print "Examples:\n";
    print "  Run all tests:\n";
    print "    cth\n";
    print "  Run only specific tests (name filters):\n";
    print "    cth testname1 test2 othertestname\n";
    print "  Install drivers, then run tests afterwards if installation successful:\n";
    print "    cth -i -f\n";
    print "\n";
    print "Running tests vs. installing test drivers:\n";
    print "  Installation/set-up operations like --install, --run, --clear and --reset\n";
    print "  prevent cth from running tests afterwards, unless --force is specified.\n";
    print "\n";
    print "Tests directories:\n";
    print "  A 'tests dir' is a directory whose subdirectories are all tests. A valid\n";
    print "  test subdirectory must contain an executable 'run' file that is the test\n";
    print "  program itself; the name of that subdirectory is the name of the test.\n";
    print "\n";
    print "Auto Switch Filter:\n";
    print "  Test directories with the SWITCHNAME:VALUE pattern in their name will be\n";
    print "  automatically skipped by cth if the switch SWITCHNAME is not defined with\n";
    print "  exact value VALUE. For example, 'target:hg2-dashes-are-separators' requires\n";
    print "  'cth -s target=hg2'. Values with e.g. '/' (illegal filename) not supported.\n";
    print "\n";
    print "Options:\n";
    print "\n";
    print "  --clear, -c         Clear all tests and drivers (doesn't run tests)\n";
    print "  --dump              Dump failed.log and all failed driver install.log files\n";
    print "  --findtests <d>     Scans directory <d> recursively for tests dirs, which\n";
    print "                      is the same as using -t for every tests subdirectory\n";
    print "  --force, -f         Force tests to run after all installation/set-up tasks\n";
    print "  --help, -h          Print this help\n";
    print "  --install, -i       Run 'install' on all drivers\n";
    print "                      Forwards specified switches/options to each installer\n";
    print "  --jobs, -j <n>      Start at most <n> simultaneous jobs; default: $cpus\n";
    print "  --option, -o <k=v>  Pass a '-o \"k=v\"' parameter to all tests/installers\n";
    print "                      (note: switches are preferred over options)\n";
    print "  --reset             Run 'reset' on all drivers (doesn't run tests)\n";
    print "                      WARNING: 'reset' is forceful and it will likely kill\n";
    print "                      *ALL* running nodeos processes!\n";
    print "  --run <d> <e> <p>   Call driver <d>'s executable <s> with parameters <p>\n";
    print "                      All --run commands are processed after -c and -i\n";
    print "  --switch, -s <k=v>  Pass a '--k \"v\"' parameter to all tests/installers\n";
    print "  --testsdir, -t <d>  Add <d> as a tests-to-run directory (default: 'tests')\n";
    print "                      If used multiple times, runs tests on multiple dirs\n";
    print "  --verbose, -v       Print more messages\n";
    print "\n";
    exit;
}

# --------------------------------------------------------------------------------------------
# Starting up.
# --------------------------------------------------------------------------------------------

if ($opt_verbose) { print "Working directory (test root): $root_dir\n"; }

# Ensure "local/" exists
if (! -d "$root_dir/local") { `mkdir -p $root_dir/local`; }
if (! -e "$root_dir/local/.gitkeep") { `touch $root_dir/local/.gitkeep`; }

if (@opt_run || $opt_install || $opt_reset || $opt_clear) {
    if ($opt_force) {
        print "Installation/set-up tasks detected but --force was specified. Will run tests if all installation/set-up tasks are succesful.\n";
    } else {
        print "Installation/set-up tasks detected. Will NOT run any tests.\n";
        $DO_NOT_RUN_TESTS = 1;
    }
} else {
    if ($opt_verbose) { print "Will run tests (no installation/set-up tasks detected).\n"; }
}

if ($opt_findtests) {
    # --findtests <d> expands in a bunch of --testsdir <sd> where sd is every subdirectory of d (or d itself) that contains
    #   *exclusively* (and at least one) subdirectories that are valid cth tests, that is, contain executable files named "run".
    my $abs_ft = Cwd::abs_path($opt_findtests);
    if (!defined $abs_ft || !-d $abs_ft) {
        print "ERROR: --findtests directory not found: $opt_findtests (current: $root_dir)\n";
        exit 1;
    }
    if ($opt_verbose) { print "Scanning for tests directories under '$abs_ft' due to --findtests ...\n"; }
    my @found_tests_dirs;
    my @stack = ($abs_ft);
    while (@stack) {
        my $dir = pop @stack;
        if (-d $dir) {
            my @subdirs;
            opendir(my $dh, $dir) or die "Cannot open directory $dir: $!";
            while (my $subdir = readdir($dh)) {
                next if $subdir eq '.' || $subdir eq '..';
                my $subdir_path = "$dir/$subdir";
                if (-d $subdir_path) {
                    push @stack, $subdir_path;
                    push @subdirs, $subdir_path;
                }
            }
            closedir($dh);
            my $all_subdirs_have_run_file = scalar(@subdirs) > 0; # must have at least 1 test subdir
            foreach my $subdir (@subdirs) {
                my $run_file = "$subdir/run";
                if (!-e $run_file || !-x $run_file) {
                    $all_subdirs_have_run_file = 0;
                    last;
                }
            }
            if ($all_subdirs_have_run_file) {
                push @found_tests_dirs, $dir; # for verbose printing only
                push @opt_testsdir, $dir;     # apply it (emulate -t <dir>)
            }
        }
    }
    if ($opt_verbose) {
        print "Found " . scalar(@found_tests_dirs) . " tests dirs";
        if (scalar(@found_tests_dirs) == 0) {
            print ".\n";
        } else {
            print ": ";
            foreach my $dir (@found_tests_dirs) { print "$dir "; }
            print "\n";
        }
    }
}

if (! @opt_testsdir) {
    push @opt_testsdir, $default_tests_dir;
    if ($opt_verbose) { print "No --testsdir, -t options detected; will run tests in default tests subdirectory '$default_tests_dir'\n"; }
}

foreach my $opt_td (@opt_testsdir) {
    my $abs_td = Cwd::abs_path($opt_td);
    if (!defined $abs_td || !-d $abs_td) {
        print "ERROR: Tests directory not found: $opt_td (current: $root_dir)\n";
        exit 1;
    }
    push @tests_dirs, $abs_td;
}

if ($opt_verbose) {
    my $tests_dir_i = 1;
    my $tests_dir_max = scalar(@tests_dirs);
    foreach my $td (@tests_dirs) {
        print "Tests directory ($tests_dir_i/$tests_dir_max): $td\n";
        $tests_dir_i++;
    }
}

#--------------------------------------------------------------------------------------------
# Refuse to run if another instance of cth is already running in the system.
# --------------------------------------------------------------------------------------------

my @pgrep_output = `pgrep -x cth`; # Run pgrep to get a list of PIDs for the "cth" process
die "ERROR: cth: Failed to run pgrep: $!" if $?;
chomp(@pgrep_output);
@pgrep_output = grep { $_ != $$ } @pgrep_output; # Remove our own PID from the list
die "ERROR: cth: This instance ($$) will exit since there are other running cth instances (PIDs: " . join(", ", @pgrep_output) . ")" if (@pgrep_output);

# --------------------------------------------------------------------------------------------
# Resolve switches and options to be passed to tests and/or installers
# --------------------------------------------------------------------------------------------

sub get_switch_value {
    my $switchname = shift;
    if (! defined $switchname) { die "ERROR: has_switch: undefined switchname"; };
    foreach my $pair (@opt_switch) {
        my ($key, $value) = @$pair;
        if ($key eq $switchname) {
            return $value;
        }
    }
    return; # undef
}

print "Global parameters given to all tests/installers:\n\n";
foreach my $pair (@opt_switch) {
    my ($key, $value) = @$pair;
    print "  Switch: \"$key\" = \"$value\" (--$key \"$value\")\n";
    if ($global_test_or_installer_args ne '') { $global_test_or_installer_args .= ' '; }
    $global_test_or_installer_args .= qq|--$key "$value"|;
}
foreach my $pair (@opt_option) {
    my ($key, $value) = @$pair;
    print "  Option: \"$key\" = \"$value\" (-o \"$key=$value\")\n";
    if ($global_test_or_installer_args ne '') { $global_test_or_installer_args .= ' '; }
    $global_test_or_installer_args .= qq|-o "$key=$value"|;
}
if ($global_test_or_installer_args eq '') {
    print "  No parameters to be passed to tests/installers (neither switches nor options).\n\n";
} else {
    print "\n";
    if ($opt_verbose) { print "Parameter string for all tests/installers:\n  $global_test_or_installer_args\n\n"; }
}

# --------------------------------------------------------------------------------------------
# Get a list of all drivers
# --------------------------------------------------------------------------------------------

{
    opendir(my $dh, $drivers_dir) or die "ERROR: cth: Cannot open drivers directory $drivers_dir: $!";
    if ($opt_verbose) { print "Searching for drivers in $root_dir/$drivers_dir ...\n"; }
    while (my $entry = readdir($dh)) {
        next if $entry eq '.' || $entry eq '..';
        push @drivers, $entry if -d "$drivers_dir/$entry";
    }
    closedir($dh);

    if (scalar(@drivers) == 0) {
        print "WARNING: No driver directories found.\n";
    } else {
        @drivers = sort @drivers;
        if ($opt_verbose) {
            print "Found drivers: ";
            my $first = 1;
            foreach my $driver (@drivers) {
                if (!$first) { print ", "; } else { $first = 0; }
                print "$driver";
            }
            print "\n";
        }
    }
}

# --------------------------------------------------------------------------------------------
# Check if we are going to --reset all drivers
# --------------------------------------------------------------------------------------------

if ($opt_reset) {
    if ($opt_verbose) { print "Running 'reset' on all drivers ...\n"; }
    foreach my $driver (@drivers) {
        my $work_dir = "$drivers_dir/$driver";
        chdir $work_dir or die "ERROR: cth: Cannot change into driver directory: $!";
        my $script = "reset";
        if (-e $script) {
            my $log_file = "reset.log";
            unlink $log_file;
            print "Running reset script for driver $driver (output: $work_dir/$log_file) ...\n";
            my $ret = system("./" . $script . " > $log_file 2>&1");
            if ($ret) {
                print "Driver $driver reset failed with code: $ret\n";
                push @FAILED_SETUP_TASKS, "$work_dir/$script";
            } else {
                print "Driver $driver reset successfully.\n";
            }
        } else {
            print "Driver $driver has no reset script.\n";
        }
        chdir $root_dir or die "ERROR: cth: Cannot change to root test directory: $!";
    }
    print "\n";
}

# --------------------------------------------------------------------------------------------
# Check if we are going to --install all drivers
# --------------------------------------------------------------------------------------------

if ($opt_install) {

    #
    # Fetch all driver installation dependencies
    #
    if ($opt_verbose) { print "Fetching driver installation dependencies...\n"; }
    my %deps;
    foreach my $driver (@drivers) {
        my $depends = "$drivers_dir/$driver/depends";
        if (-e $depends) {
            open my $fh, '<', $depends or die "ERROR: cth: Cannot open driver '$driver' depends file '$depends': $!";
            my @driver_deps;
            while (<$fh>) {
                chomp;
                $_ =~ s/^\s+|\s+$//g;  # Trim leading and trailing whitespace
                # Ignore lines with pure whitespace
                next if $_ eq '';
                # Check if the string is a valid POSIX directory name
                if ($_ =~ /^[a-zA-Z0-9_.-]+$/) {
                    if ($opt_verbose) { print "Driver '$driver' depends on driver '$_'\n"; }
                    push @driver_deps, $_;
                } else {
                    print "ERROR: cth: Invalid driver '$driver' dependency directory name: $_\n";
                    exit 1;
                }
            }
            $deps{$driver} = \@driver_deps;
            close $fh;
        }
    }

    #
    # Resolve driver installation dependencies
    #
    if ($opt_verbose) { print "Resolving driver installation dependencies...\n"; }
    my @sorted_drivers = @drivers;
    my $inconsistent = 1;
    my $max_iters = scalar(@drivers) * scalar(@drivers);
    while ($inconsistent) {
        $inconsistent = 0;
        for my $i (0 .. $#sorted_drivers) {
            for my $j ($i + 1 .. $#sorted_drivers) {
                my $driver1 = $sorted_drivers[$i];
                my $driver2 = $sorted_drivers[$j];
                if (defined $deps{$driver1} && grep { $_ eq $driver2 } @{$deps{$driver1}}) {
                    # Swap drivers if driver1 depends on driver2
                    ($sorted_drivers[$i], $sorted_drivers[$j]) = ($sorted_drivers[$j], $sorted_drivers[$i]);
                    $inconsistent = 1;
                }
            }
        }
        $max_iters -= 1;
        if ($max_iters <= 0 && $inconsistent) {
            print "ERROR: cth: drivers likely have circular dependencies; can't install them.\n";
            exit 1;
        }
    }
    if ($opt_verbose) {
        print "Original driver list: @drivers\n";
        print "Resolved driver list: @sorted_drivers\n";
    }
    @drivers = @sorted_drivers;

    #
    # Install all drivers
    #
    print "Running 'install' on all drivers ...\n";
    foreach my $driver (@drivers) {
        my $work_dir = "$drivers_dir/$driver";
        chdir $work_dir or die "ERROR: cth: Cannot change into driver directory: $!";
        my $script = "install";
        if (-e $script) {

            # before running install, see if the driver has install notes and print those
            my $install_notes_file = 'README-INSTALL.md';
            if (-e $install_notes_file) {
                print "Driver $driver has installation notes ('$install_notes_file'):\n";
                print "--------------------------------------------------------------------------------\n";
                if (open(my $fh, '<', $install_notes_file)) {
                    while (my $line = <$fh>) { print $line; }
                    close($fh);
                } else {
                    print "WARNING: Cannot open '$install_notes_file'.\n";
                }
                print "--------------------------------------------------------------------------------\n";
            }

            # run 'install'
            my $log_file = "install.log";
            unlink $log_file;
            print "Running install script for driver $driver (output: $work_dir/$log_file) ...\n";
            my $ret = system("./$script $global_test_or_installer_args  > $log_file 2>&1");
            if ($ret) {
                print "*******************************************************************************\n";
                print "WARNING: cth: Driver $driver installation failed with code: $ret\n";
                print "              This will probably cause any tests that use this driver to fail.\n";
                print "*******************************************************************************\n";
                push @FAILED_SETUP_TASKS, "$work_dir/$script";

                if ($opt_dump) {
                    print "\n";
                    print "--------------------------------------------------------------------------------\n";
                    print "Begin dump: $work_dir/$log_file\n";
                    print "--------------------------------------------------------------------------------\n";
                    print `cat $log_file`;
                    print "--------------------------------------------------------------------------------\n";
                    print "End dump: $work_dir/$log_file\n";
                    print "--------------------------------------------------------------------------------\n";
                    print "\n";
                }
            } else {
                print "Driver $driver installed successfully.\n";
            }
        } else {
            print "Driver $driver has no install script.\n";
        }
        chdir $root_dir or die "ERROR: cth: Cannot change to root test directory: $!;"
    }

    print "\n";
}

# --------------------------------------------------------------------------------------------
# Find all tests.
# --------------------------------------------------------------------------------------------

foreach my $tests_dir (@tests_dirs) {

    my @tests_found; # tests found in this tests dir

    opendir(my $dh, $tests_dir) or die "ERROR: cth: Cannot open tests directory $tests_dir: $!";
    if ($opt_verbose) { print "Searching for tests in $tests_dir ...\n"; }
    while (my $entry = readdir($dh)) {
        next if $entry eq '.' || $entry eq '..';
        my $test_path = "$tests_dir/$entry";
        if (-d $test_path) {
            # only consider a directory to be a test directory if it has a 'run' file that is marked executable
            my $run_file = "$test_path/run";
            if (-e $run_file && -x $run_file) {
                push @tests_found, $test_path;
            }
        }
    }
    closedir($dh);

    if (scalar(@tests_found) == 0) {
        print "WARNING: No test directories found in tests dir $tests_dir\n";
    } else {
        @tests_found = sort @tests_found;
        if ($opt_verbose) {
            print "Found " . scalar(@tests_found) . " tests: ";
            my $first = 1;
            foreach my $test (@tests_found) {
                if (!$first) { print ", "; } else { $first = 0; }
                print basename($test);
            }
            print "\n";
        }
        push @tests, @tests_found; # this saves the tests found in this testsdir
    }
}

$tests_size = scalar(@tests);
@tests = sort @tests;

# --------------------------------------------------------------------------------------------
# Compute the size of the path prefix that is common throughout all entries of @tests.
# This removes redundant on-screen info while running tests.
# --------------------------------------------------------------------------------------------

if (@tests) {
    $tests_prefix = $tests[0];
    foreach my $test (@tests) {
        my @test_parts = File::Spec->splitdir($test);
        my @common_parts = File::Spec->splitdir($tests_prefix);
        my $min_parts = @test_parts < @common_parts ? @test_parts : @common_parts;
        my $match_parts = 0;
        for my $i (0 .. $min_parts - 1) {
            last if $test_parts[$i] ne $common_parts[$i];
            $match_parts++;
        }
        $tests_prefix = File::Spec->catdir(@test_parts[0 .. $match_parts - 1]);
    }
    # include a last slash into the common prefix if it's there
    if (length($tests_prefix) < length($tests[0])) {
        my $index = length($tests_prefix);
        my $char = substr($tests[0], $index, 1);
        if ($char eq '/') { $tests_prefix .= '/'; }
    }
    if ($opt_verbose) { print "Common test name prefix (omitted) is: '$tests_prefix'\n"; }
}

# utility that returns the shortest name a test can have in this run that would be disambiguated
#  (not *exactly*, since a test run with 'dir1/test1' and 'dir2/test2' would still return those;
#   what it gets rid of is the redundant /a/b/c path e.g. /a/b/c/dir1/test1 and /a/b/c/dir2/test2,
#   even though you could deduce test1 is in dir1 and test2 is in dir2).
sub shorten_test_name {
    my $test_name = shift;
    if (! defined $test_name) { die "FATAL ERROR: cth: shorten_test_name undefined test_name"; }
    if (length($test_name) >= length($tests_prefix)) { $test_name = substr($test_name, length($tests_prefix)); }
    return $test_name;
}

# --------------------------------------------------------------------------------------------
# Check if we are going to --clear all tests and drivers
# --------------------------------------------------------------------------------------------

if ($opt_clear) {

    print "Clearing all drivers ...\n";
    foreach my $driver (@drivers) {
        my $work_dir = "$drivers_dir/$driver";
        chdir $work_dir or die "ERROR: cth: Cannot change into driver directory: $!";
        my $script = "clear";
        print "Clearing driver $driver ... ";
        unlink "install.log"; # remove install.log file
        if (-e $script) {
            my $ret = system("./" . $script);
            if ($ret) {
                print "ERROR: 'clear' failed for driver $driver with code: $ret\n";
                push @FAILED_SETUP_TASKS, "$work_dir/$script";
            } else {
                print "\n";
            }
        } else {
            if ($opt_verbose) { print "(no custom 'clear' file)"; }
            print "\n";
        }
        chdir $root_dir or die "ERROR: cth: Cannot change to root test directory: $!;"
    }

    print "Clearing all tests ...\n";
    foreach my $test (@tests) {
        chdir $test or die "ERROR: cth: Cannot change into test directory: $!";
        my $script = "clear";
        print "Clearing test $test ... ";
        unlink "run.log"; # remove run.log file
        if (-e $script) {
            my $ret = system("./" . $script);
            if ($ret) {
                print "ERROR: 'clear' failed for test $test with code: $ret\n";
                push @FAILED_SETUP_TASKS, "$test/$script";
            } else {
                print "\n";
            }
        } else {
            if ($opt_verbose) { print "(no custom 'clear' file)"; }
            print "\n";
        }
        chdir $root_dir or die "ERROR: cth: Cannot change to root test directory: $!;"
    }

    print "\n";
}

# --------------------------------------------------------------------------------------------
# Process all --run commands
# --------------------------------------------------------------------------------------------

while (scalar(@opt_run) > 0) {

    if (scalar(@opt_run) < 3) {
        die "ERROR: --run option should have exactly 3 arguments; too few arguments.\n";
    }
    my $driver = shift @opt_run;
    my $executable = shift @opt_run;
    my $parameters = shift @opt_run;

    print "cth: Running driver:'$driver' executable:'$executable' parameters:'$parameters' ...\n";

    my $dir;
    if (grep { $_ eq $driver } @drivers) {
        $dir = "drivers/$driver";
    } else {
        die "ERROR: cth: cannot --run driver '$driver': no such driver found.\n";
    }

    if (! -d $dir) {
        die "ERROR: cth: cannot --run driver '$driver': directory not found: '$dir'\n";
    }

    my $exe = "$dir/$executable";

    if (! -e $exe) {
        die "ERROR: cth: cannot --run driver '$driver' executable '$executable': executable not found at: '$exe'\n";
    }

    if (! -x $exe) {
        die "ERROR: cth: cannot --run driver '$driver' executable '$executable': executable isn't: '$exe'\n";
    }

    my $cmd = "$exe $parameters";
    $cmd =~ s/\s+$//; # remove whitespace

    my $ret = system("./" . "$cmd 2>&1");
    if ($ret) {
        die "ERROR: cth: system('$cmd') returned error: $ret\n";

        # We are dying on any failed --run, so this is pointless.
        # --run is really the user doing manual installations, so in that case we want to fail-fast for sure.
        #push @FAILED_SETUP_TASKS, $cmd;
    }

    print "cth: --run '$cmd' finished successfully.\n\n";
}

# --------------------------------------------------------------------------------------------
# Decide whether we are going to run tests or not
# --------------------------------------------------------------------------------------------

if (@FAILED_SETUP_TASKS) {
    print "ERROR: Some installation/set-up tasks have failed.\n";
    foreach my $failed_setup_task (@FAILED_SETUP_TASKS) {
        print "  Failed: $failed_setup_task\n";
    }
    print "\n";
    if (! $DO_NOT_RUN_TESTS) {
        print "ERROR: Tests were scheduled to run (due to --force), but will be ignored due to installation/set-up tasks having failed.\n";
        print "\n";
    }
    exit 1;
}

if ($DO_NOT_RUN_TESTS) {
    print "\n";
    print "Test running is disabled because installation/set-up tasks were performed.\n";
    print "  (to force test running after installation/set-up tasks, use --force)\n";
    print "\n";
    exit;
}

# --------------------------------------------------------------------------------------------
# Configure environment variables so the tests can find the test libraries under tools/
# --------------------------------------------------------------------------------------------

my %seen;
my @dirs_to_process;

# Javascript test support settings (specifically the Node.js javascript interpreter).
#
# Add every tool subdirectory to NODE_PATH that has a .js file in it.

my @js_directories;
@dirs_to_process = ("$root_dir/tools");
while (my $dir = shift @dirs_to_process) {
    opendir my $dh, $dir or die "Unable to open directory $dir: $!";
    my @files = readdir $dh;
    closedir $dh;
    foreach my $file (@files) {
        next if $file eq '.' or $file eq '..';
        my $path = "$dir/$file";
        if (-d $path) {
            push @dirs_to_process, $path;
        } elsif (-f $path && $file =~ /\.js$/) {
            my $js_directory = $dir;
            push @js_directories, $js_directory;
        }
    }
}
@js_directories = grep { !$seen{$_}++ } @js_directories; # Remove duplicate directories
if (defined $ENV{NODE_PATH} && $ENV{NODE_PATH} ne '') {
    $ENV{NODE_PATH} .= ':' . join(':', @js_directories);
} else {
    $ENV{NODE_PATH} = join(':', @js_directories);
}
if ($opt_verbose) { print "Node.js tests will receive environment variable NODE_PATH: $ENV{NODE_PATH}\n"; }

# Perl test support settings.
#
# Add every tool subdirectory to @INC that has a .pm file in it (add the previous
#   one as well so we can do e.g. "use JSON::Tiny;" instead of "use Tiny;")
# This is needed because we don't know how deep under tools/ the .pm files are.
my @pm_directories;
@dirs_to_process = ("$root_dir/tools");
while (my $dir = shift @dirs_to_process) {
    opendir my $dh, $dir or die "Unable to open directory $dir: $!";
    my @files = readdir $dh;
    closedir $dh;
    foreach my $file (@files) {
        next if $file eq '.' or $file eq '..';
        my $path = "$dir/$file";
        if (-d $path) {
            push @dirs_to_process, $path;
        } elsif (-f $path && $file =~ /\.pm$/) {
            my $pm_directory = $dir;
            push @pm_directories, $pm_directory;
            push @pm_directories, dirname($pm_directory);
        }
    }
}
@pm_directories = grep { !$seen{$_}++ } @pm_directories; # Remove duplicate directories
if (defined $ENV{PERL5LIB} && $ENV{PERL5LIB} ne '') {
    $ENV{PERL5LIB} .= ':' . join(':', @pm_directories);
} else {
    $ENV{PERL5LIB} = join(':', @pm_directories);
}
if ($opt_verbose) { print "Perl tests will receive environment variable PERL5LIB: $ENV{PERL5LIB}\n"; }

# --------------------------------------------------------------------------------------------
# Run tests
# --------------------------------------------------------------------------------------------

sub print_test_name {
    my $test_name = shift;
    if (! defined $test_name) { die "FATAL ERROR: cth: print_test_name undefined test_name"; }
    $test_name = shorten_test_name($test_name);
    my $tests_done  = $sum_pass + $sum_fail + $sum_skip;
    my $test_counter = sprintf("%3d", int(100 * (($tests_done + 1) / $tests_size))) . "%";
    print "  " . $test_counter . " " . $test_name . " " . ("." x (50 - length($test_name) - 1 - length($test_counter))) . " ";
}

sub test_result {
    my $test_name = shift;
    if (! defined $test_name) { die "FATAL ERROR: cth: test_result undefined test_name"; }
    $test_name = shorten_test_name($test_name);
    my $exit_status = shift;
    if (! defined $exit_status) { die "FATAL ERROR: cth: test_result undefined exit_status"; }
    if ($exit_status == 0) {
        print "${GRN}Passed${DEF}.\n";
        $sum_pass++;
        push @sum_passed_tests, $test_name;
    } elsif ($exit_status == $cth_exit_code_skipped_test) {
        print "Skipped.\n";
        $sum_skip++;
        push @sum_skipped_tests, $test_name;
    } else {
        print "${RED}Failed${DEF} (exit code: $exit_status).\n";
        $sum_fail++;
        push @sum_failed_tests, $test_name;
    }
}

sub wait_and_process_test_result {
    if ($opt_verbose) { print "Waiting (running $active_jobs/$opt_jobs)... "; }
    my $pid = waitpid(-1, 0);  # Wait for any child process to finish and get its PID
    if ($opt_verbose) { print "\n"; }
    if ($pid > 0) {
        $active_jobs--;
        my $test_name = delete $child_pids{$pid};
        my $exit_status = $? >> 8;  # Extract the exit status from the child process
        print_test_name($test_name);
        test_result($test_name, $exit_status);
    } else {
        print "ERROR: cth: waitpid() returned <= 0 (should never happen).\n";
    }
}

#
# First, apply the test name filter (from the command-line)
# This actually changes the @tests array
#

if (scalar(@testspecs) != 0) {

    # testspecs (test name filters) were passed in the command-line,
    #   so we have to apply those filters now.

    my @non_filtered_tests;

    foreach my $test (@tests) {
        my $shortened_test_name = shorten_test_name($test);
        if (!grep { $_ eq $shortened_test_name } @testspecs) {
            push @filtered_tests, $test;
        } else {
            push @non_filtered_tests, $test;
        }
    }

    if ($opt_verbose) {
        if (@filtered_tests) {
            print "Will not run " . scalar(@filtered_tests) . " tests that did not pass the name filters:";
            foreach my $test (@filtered_tests) {
                $test = shorten_test_name($test);
                print " " . $test;
            }
            print "\n";
        }
    }

    @tests = @non_filtered_tests; # update @tests
    $tests_size = scalar(@tests); # update size of tests
}

#
# Second, make sure failed.log is gone
#

unlink($failed_summary_log_file);

#
# Third, actually run the test list (after the test name filter)
#

if ($opt_verbose) { print "\n"; }
print "Running $tests_size test";
if ($tests_size > 1) {
    print "s";
    if ($opt_jobs > 1) {
        print " (";
        if ($tests_size > $opt_jobs) { print "up to $opt_jobs "; }
        print "in parallel)";
    }
}
print " ...\n";
if ($opt_verbose) { print "The output of each test is saved to the 'run.log' file on each test directory.\n"; }
print "\n";

# NOTE: @tests now contains the full path to every test directory, e.g. '/home/username/cth/tests/combat-test'
foreach my $test (@tests) {

    my $short_test_name = shorten_test_name($test);

    my $chdir_result = chdir $test;
    if (! $chdir_result) {
        print_test_name($test);
        print "${RED}Failed${DEF} ($test: $!)\n";
        $sum_fail++;
        push @sum_failed_tests, $short_test_name;
        next;
    }

    my $script = "run";
    my $log_file = "run.log";
    unlink $log_file;

    # pass test name through the auto switch filter
    my $auto_switch_filter_skip_test = 0;
    my $tmp = $test;
    while ($tmp =~ /(?:^|\W)(\w+:\w+)(?:\W|$)/g) {
        my ($switchname, $switchvalue) = split /:/, $1;
        my $actualvalue = get_switch_value($switchname);
        if (!defined $actualvalue) {
            print_test_name($test);
            print "Skipped (missing switch '$switchname')\n";
            $auto_switch_filter_skip_test = 1;
            last;
        }
        if ($actualvalue ne $switchvalue) {
            print_test_name($test);
            print "Skipped (switch '$switchname' != '$switchvalue')\n";
            $auto_switch_filter_skip_test = 1;
            last;
        }
    }

    # tally and/or run test
    if ($auto_switch_filter_skip_test) {
        $sum_skip++;
        push @sum_skipped_tests, $short_test_name;
    } elsif (! -e $script) {
        print_test_name($test);
        print "${RED}Failed${DEF} (no 'run' file)\n";
        $sum_fail++;
        push @sum_failed_tests, $short_test_name;
    } else {
        if ($active_jobs >= $opt_jobs) {
            if ($opt_verbose) { print_test_name($test); }
            wait_and_process_test_result();
        }

        my $pid = fork();

        # Even if the fork failed, we can try executing the test in the cth parent process
        if (!defined $pid) {
            print "Fork failed... ";
        }

        if (!defined $pid || $pid == 0) {
            if (!defined $pid) {
                print "Started... ";
            }
            my $ret = system("./" . "$script $global_test_or_installer_args > $log_file 2>&1");
            if ($ret) {
                if ($ret < 0) {
                    if (!defined $pid) {
                        print "${RED}Failed${DEF} (system call: $ret)\n";
                        $sum_fail++;
                        push @sum_failed_tests, $short_test_name;
                    } else {
                        exit(255);
                    }
                } else {
                    my $exit_status = $ret >> 8;
                    if (!defined $pid) {
                        if ($exit_status == 0) { $exit_status = 254; } # never happens, but let's ensure
                        test_result($test, $exit_status);
                    } else {
                        exit($exit_status);
                    }
                }
            } else {
                if (!defined $pid) {
                    test_result($test, 0);
                } else {
                    exit(0);
                }
            }
        }

        # if pid is not defined, fork failed so we ran the test in the parent cth process
        if (defined $pid) {
            if ($opt_verbose) {
                print_test_name($test);
                print "Running (PID $pid)...\n";
            }
            $active_jobs++;
            $child_pids{$pid} = $test;
        }
    }

    chdir $root_dir or die "ERROR: cth: Cannot change to root test directory: $!;"
}

# Wait for any remaining child processes to finish
while ($active_jobs > 0) {
    if ($opt_verbose) { print_test_name(''); }
    wait_and_process_test_result();
}

# --------------------------------------------------------------------------------------------
# Output failed tests summary to failed.log
# --------------------------------------------------------------------------------------------

if ($sum_fail > 0 && open(my $failed_log, '>', $failed_summary_log_file)) {

    print $failed_log "cth error summary file for ${sum_fail} failed tests:\n";
    foreach my $test (@sum_failed_tests) {
        print $failed_log "  ${tests_prefix}${test}\n";
    }

    my $failure_context_size = 7; # number of lines before and after an error line to include
    my $count = 0;
    foreach my $test (@sum_failed_tests) {
        $count++;
        my $testrunlog = "${tests_prefix}${test}/run.log";

        print $failed_log "\n${GRN}";
        print $failed_log "------------------------------------------------------------------------------\n";
        print $failed_log "$testrunlog ($count/$sum_fail)\n";
        print $failed_log "------------------------------------------------------------------------------\n";
        print $failed_log "${DEF}\n";

        my $r = open(my $run_log, '<', $testrunlog);
        if (! $r) {
            print $failed_log "${RED}[Failed to open '$testrunlog': $!]${DEF}\n";
            next;
        }
        my @run_log_lines = <$run_log>;
        close($run_log);

        my %lines_to_print;
        for (my $i = 0; $i < scalar(@run_log_lines); $i++) {
            $lines_to_print{$i} = 0;
        }

        for (my $i = 0; $i < scalar(@run_log_lines); $i++) {
            my $line = $run_log_lines[$i];
            if ($line =~ /error:|warning:/i) {
                $line =~ s/(error:|warning:)/${RED}${1}${DEF}/gi;
                $run_log_lines[$i] = $line;
                $lines_to_print{$i} = 1;
                for my $j (reverse 0..$i) {
                    last if $j < $i - $failure_context_size;
                    $lines_to_print{$j} = 1;
                }
                for my $j ($i + 1..$i + $failure_context_size) {
                    last if $j >= scalar(@run_log_lines);
                    $lines_to_print{$j} = 1;
                }
            }
        }

        my $output = "";
        my $omit_start = -1;
        for (my $i = 0; $i < scalar(@run_log_lines); $i++) {
        if ($lines_to_print{$i} == 0) {
            if ($omit_start == -1) {
                $omit_start = $i;
            }
            next;
        }
        if ($omit_start != -1) {
            $output .= "${YEL}[$omit_start ... " . ($i - 1) . "]$DEF\n";
            $omit_start = -1;
        }
        $output .= $YEL . sprintf("%4d", $i) . "|" . $DEF . $run_log_lines[$i];
        }
        if ($omit_start != -1) {
            $output .= "${YEL}[$omit_start ... " . scalar(@run_log_lines) . "]$DEF\n";
        }
        print $failed_log $output;
    }
    close($failed_log);
    if ($opt_verbose) { print "\nSummary of all failed tests written to '$root_dir/$failed_summary_log_file'.\n"; }

    if ($opt_dump) {
        print "\n";
        print "--------------------------------------------------------------------------------\n";
        print "Begin dump: $failed_summary_log_file\n";
        print "--------------------------------------------------------------------------------\n";
        print `cat $failed_summary_log_file`;
        print "--------------------------------------------------------------------------------\n";
        print "End dump: $failed_summary_log_file\n";
        print "--------------------------------------------------------------------------------\n";
        print "\n";
    }

} elsif ($sum_fail > 0) {
    print "ERROR: Failed to open '$failed_summary_log_file': $!\n";
}

# --------------------------------------------------------------------------------------------
# Print summary
# --------------------------------------------------------------------------------------------

@sum_passed_tests  = sort @sum_passed_tests;
@sum_failed_tests  = sort @sum_failed_tests;
@sum_skipped_tests = sort @sum_skipped_tests;

$sum_total = $sum_pass + $sum_fail + $sum_skip;

$sum_pass  = sprintf("%4d", $sum_pass);
$sum_fail  = sprintf("%4d", $sum_fail);
$sum_skip  = sprintf("%4d", $sum_skip);
$sum_total = sprintf("%4d", $sum_total);

print "\n";
print "-------------\n";
print "Test summary:\n";
print "-------------\n";
print "\n";

my $name_filter_skip = scalar(@filtered_tests);
if ($name_filter_skip > 0) {
    print "Filtered $name_filter_skip tests by name (not included in stats below).\n";
    print "\n";
}

print "Passed     : $sum_pass\n";
print "Failed     : $sum_fail\n";
print "Skipped    : $sum_skip\n";
print "-----------------\n";
print "Total      : $sum_total\n";

if ($sum_pass > 0) {
    print "\n";
    print "Passed tests: ";
    my $first = 1;
    foreach my $passed_test (@sum_passed_tests) {
        if (!$first) { print ", "; } else { $first = 0; }
        print $passed_test;
    }
    print "\n";
}

if ($sum_fail > 0) {
    print "\n";
    print "Failed tests: ";
    my $first = 1;
    foreach my $failed_test (@sum_failed_tests) {
        if (!$first) { print ", "; } else { $first = 0; }
        print $failed_test;
    }
    print "\n";
}

if ($sum_skip > 0) {
    print "\n";
    print "Skipped tests: ";
    my $first = 1;
    foreach my $skipped_test (@sum_skipped_tests) {
        if (!$first) { print ", "; } else { $first = 0; }
        print $skipped_test;
    }
    print "\n";
}

print "\n";

if ($sum_fail > 0) {
    print "RESULT: FAILED\n";
    exit 1;
} elsif ($sum_pass > 0) {
    print "RESULT: PASSED\n";
} else {
    print "RESULT: NONE\n";
}
